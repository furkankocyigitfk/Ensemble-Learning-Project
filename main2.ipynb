{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGH4MVKYxzwc",
        "outputId": "fb039181-6a43-45ba-d0ca-945374ef4b39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "os.chdir(\"drive/MyDrive/Kolektif Öğrenme/project\")\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc_Rdc5Hx4nz",
        "outputId": "c7223719-b500-404e-aa1e-4720da3d616f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/Kolektif Öğrenme/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGMHiqNzyNH7",
        "outputId": "bdc6f180-ca80-4c4c-d9ea-1a408199bf9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30534 sha256=0952533477e4a3c82c00fc86fc76c20cd3d921a3e1c64d61ef005028eac22789\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/a4/72/df07592cea3ae06b5e846f5e52262f8b16748e829ca354b7df\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=177f7d7b48de4f089fdbe04457e23b11a2e36b7c730f64d1c3afe77c22970af6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f3/85/b8cf1d8bfe55dc2ece0f1fcd4e91d6f8fc7b59ff3fd75329e1\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=38f361ac09ab6596e1197c691175fc25aded06abcc78b0487ae15b31bab2b90a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/26/e9/df16869ccbd4abf517f1ff3be9a2c7ee5c5980fc87eea04fb1\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dciUR7kpxs-H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, MaxPooling2D, Flatten, Dropout, Input, Lambda\n",
        "from keras.models import Model\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "import math\n",
        "from itertools import combinations as comb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7NnEuMtxs-Y",
        "outputId": "572d3a85-b28d-401f-8d97-787275e10eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "81MKP_Dbxs-f"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"./Sarcasm_Headlines_Dataset_v2.json\", lines= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qbEQAwGSxs-j",
        "outputId": "35d451eb-edcf-4910-fbfb-4fc8079818eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   is_sarcastic                                           headline  \\\n",
              "0             1  thirtysomething scientists unveil doomsday clo...   \n",
              "1             0  dem rep. totally nails why congress is falling...   \n",
              "2             0  eat your veggies: 9 deliciously different recipes   \n",
              "3             1  inclement weather prevents liar from getting t...   \n",
              "4             1  mother comes pretty close to using word 'strea...   \n",
              "\n",
              "                                        article_link  \n",
              "0  https://www.theonion.com/thirtysomething-scien...  \n",
              "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
              "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
              "3  https://local.theonion.com/inclement-weather-p...  \n",
              "4  https://www.theonion.com/mother-comes-pretty-c...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36adabde-ab6e-498f-8240-3e0b9c67b0d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36adabde-ab6e-498f-8240-3e0b9c67b0d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36adabde-ab6e-498f-8240-3e0b9c67b0d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36adabde-ab6e-498f-8240-3e0b9c67b0d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8X-N5wFxs-s",
        "outputId": "68f563b0-e6ff-489d-d72c-85d69f7b0bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28619 entries, 0 to 28618\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   is_sarcastic  28619 non-null  int64 \n",
            " 1   headline      28619 non-null  object\n",
            " 2   article_link  28619 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 670.9+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HfzH3LJtxs-u"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GIN2JnBQxs-2"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
        "\n",
        "def encoder(sentences):\n",
        "  ids = []\n",
        "  for sentence in sentences:\n",
        "    encoding = tokenizer.encode_plus(\n",
        "    sentence,\n",
        "    max_length=16,\n",
        "    truncation = True,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=False)\n",
        "    ids.append(encoding['input_ids'])\n",
        "  return ids\n",
        "\n",
        "train_sents,test_sents, train_labels, test_labels  = train_test_split(df[\"headline\"],df[\"is_sarcastic\"], test_size=0.2)\n",
        "\n",
        "train_ids = encoder(train_sents)\n",
        "test_ids = encoder(test_sents) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HonSkqFExs-8"
      },
      "outputs": [],
      "source": [
        "train_ids = tf.convert_to_tensor(train_ids)\n",
        "test_ids = tf.convert_to_tensor(test_ids)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "train_labels = tf.convert_to_tensor(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9HGFaFfxs--",
        "outputId": "c0fae749-9bfb-42d6-f9c7-5c376acf7283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "input_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \n",
        "embedding = bert_encoder([input_word_ids])\n",
        "dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n",
        "dense = tf.keras.layers.Dense(128, activation='relu',kernel_regularizer='l2')(dense)\n",
        "dense = tf.keras.layers.Dropout(0.2)(dense)   \n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid',kernel_regularizer='l2')(dense)    \n",
        "\n",
        "model = tf.keras.Model(inputs=[input_word_ids], outputs=output)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1CLwJ4dxs_A",
        "outputId": "dd66f623-abbf-401a-bb2b-64cfebca6575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358/358 [==============================] - 111s 260ms/step - loss: 2.4811 - accuracy: 0.5192 - val_loss: 2.3676 - val_accuracy: 0.5168\n",
            "Epoch 2/5\n",
            "358/358 [==============================] - 90s 251ms/step - loss: 2.2634 - accuracy: 0.5220 - val_loss: 2.1630 - val_accuracy: 0.5168\n",
            "Epoch 3/5\n",
            "358/358 [==============================] - 90s 250ms/step - loss: 2.0694 - accuracy: 0.5255 - val_loss: 1.9798 - val_accuracy: 0.5168\n",
            "Epoch 4/5\n",
            "358/358 [==============================] - 90s 250ms/step - loss: 1.8959 - accuracy: 0.5247 - val_loss: 1.8155 - val_accuracy: 0.5168\n",
            "Epoch 5/5\n",
            "358/358 [==============================] - 89s 249ms/step - loss: 1.7405 - accuracy: 0.5250 - val_loss: 1.6687 - val_accuracy: 0.5168\n"
          ]
        }
      ],
      "source": [
        "model.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x = train_ids, y = train_labels, epochs = 5, verbose = 1, batch_size = 64, validation_data = (test_ids, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H-yewuaxs_C",
        "outputId": "89f1049a-dafb-4f2b-8936-fed7cb32414c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_word_ids (InputLayer)  [(None, 16)]             0         \n",
            "                                                                 \n",
            " tf_bert_model_2 (TFBertMode  TFBaseModelOutputWithPoo  109482240\n",
            " l)                          lingAndCrossAttentions(l            \n",
            "                             ast_hidden_state=(None,             \n",
            "                             16, 768),                           \n",
            "                              pooler_output=(None, 76            \n",
            "                             8),                                 \n",
            "                              past_key_values=None, h            \n",
            "                             idden_states=None, atten            \n",
            "                             tions=None, cross_attent            \n",
            "                             ions=None)                          \n",
            "                                                                 \n",
            " lambda_2 (Lambda)           (None, 768)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,482,240\n",
            "Trainable params: 109,482,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "bert_encoder = Model(input_word_ids, model.get_layer(list(filter(lambda x: \"lambda\" in x.name,  model.layers))[0].name).output)\n",
        "bert_encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYTMCrWCxs_E",
        "outputId": "d4cf8dde-a1fe-4992-c0c7-50231133a1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "716/716 [==============================] - 36s 47ms/step\n",
            "179/179 [==============================] - 9s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "train_embed = bert_encoder.predict(train_ids)\n",
        "test_embed = bert_encoder.predict(test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DWwUsBiTxs_F"
      },
      "outputs": [],
      "source": [
        "def generate_imp_space(X_train, y_train, X_test, imp_feature_size, foz):\n",
        "    imp_train_data = X_train.values\n",
        "    imp_test_data = X_test.values\n",
        "    y_train = pd.DataFrame(y_train)\n",
        "    d = len(X_train.columns)\n",
        "\n",
        "    for i in range(0, imp_feature_size*foz):\n",
        "        Xindis = np.random.permutation(d)\n",
        "        for j in range(0, d-(foz-1), foz):\n",
        "            sX = np.random.permutation(1)\n",
        "            s1 = sX[0]\n",
        "\n",
        "            s1data = X_train[X_train.index.isin(y_train[y_train == str(s1)].index)]\n",
        "            s2data = X_train[~X_train.index.isin(y_train[y_train == str(s1)].index)]\n",
        "            s1data = s1data.iloc[:, Xindis[j:j+(foz)]]\n",
        "            s2data = s2data.iloc[:, Xindis[j:j+(foz)]]\n",
        "\n",
        "            s1label = np.ones((s1data.values.shape[0], 1), dtype=int)\n",
        "            s2label = -1*np.ones((s2data.values.shape[0], 1), dtype=int)\n",
        "            Wdata = np.concatenate((s1data,s2data))\n",
        "\n",
        "            Wdata = x2fx(Wdata)\n",
        "            Wlabel = np.concatenate((s1label, s2label))\n",
        "            W = np.matmul(np.matmul(np.linalg.pinv(\n",
        "                np.matmul(Wdata.T, Wdata)), Wdata.T), Wlabel)\n",
        "\n",
        "            WW = x2fx(X_train.iloc[:, Xindis[j:j+(foz)]].values)\n",
        "            imp_train_data = np.concatenate(\n",
        "                (imp_train_data, np.matmul(WW, W)), axis=1)\n",
        "\n",
        "            TT = x2fx(X_test.iloc[:, Xindis[j:j+(foz)]].values)\n",
        "            imp_test_data = np.concatenate(\n",
        "                (imp_test_data, np.matmul(TT, W)), axis=1)\n",
        "\n",
        "    return imp_train_data, imp_test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0HNn8iPjxs_H"
      },
      "outputs": [],
      "source": [
        "def x2fx(x, model=\"linear\"):\n",
        "    linear = np.c_[np.ones(x.shape[0]), x]\n",
        "    if model == \"linear\":\n",
        "        return linear\n",
        "    if model == \"purequadratic\":\n",
        "        return np.c_[linear, x**2]\n",
        "    interaction = np.hstack([x[:, i]*x[:, j]\n",
        "                            for i, j in comb(range(x.shape[1]), 2)]).T\n",
        "    if model == \"interaction\":\n",
        "        return np.c_[linear, interaction]\n",
        "    if model == \"quadratic\":\n",
        "        return np.c_[linear, interaction, x**2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "0xKxNJUIxs_J"
      },
      "outputs": [],
      "source": [
        "foz = 4\n",
        "imp_feature_size = 1\n",
        "n_estimators = 5\n",
        "imp_acc = []\n",
        "rfc_acc = []\n",
        "estimators_imp = []\n",
        "estimators = []\n",
        "train_embed = pd.DataFrame(train_embed)\n",
        "test_embed=pd.DataFrame(test_embed)\n",
        "d = len(train_embed.columns)\n",
        "\n",
        "for i in range(n_estimators):\n",
        "    imp_tr, imp_ts = generate_imp_space(train_embed, train_labels, test_embed, imp_feature_size, foz)\n",
        "    imp_d = imp_tr.shape[1]\n",
        "\n",
        "    imp_sel_d = 2 * round(math.log2(imp_d))\n",
        "    sel_d = 2*round(math.log2(d))\n",
        "\n",
        "    imp_rfc = RandomForestClassifier(max_features=imp_sel_d, n_estimators=50, random_state=42)\n",
        "    imp_rfc.fit(imp_tr, train_labels)\n",
        "    estimators_imp.append((\"imp_rfc\"+str(i), imp_rfc))\n",
        "\n",
        "    rfc = RandomForestClassifier(max_features=sel_d, n_estimators=50, random_state=42)\n",
        "    rfc.fit(train_embed, train_labels)\n",
        "    estimators.append((\"rfc\"+str(i), rfc))\n",
        "\n",
        "\n",
        "voting_imp = VotingClassifier(estimators=estimators_imp)\n",
        "voting_imp.fit(imp_tr, train_labels)\n",
        "\n",
        "voting_rfc = VotingClassifier(estimators=estimators)\n",
        "voting_rfc.fit(train_embed, train_labels)\n",
        "\n",
        "imp_acc.append(voting_imp.score(imp_ts, test_labels))\n",
        "rfc_acc.append(voting_rfc.score(test_embed, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imp_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEuJGVsA0-zp",
        "outputId": "119f5bb5-4b6a-4fa9-8dbb-1fe612d29a2f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5435010482180294]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdjarZBJ0_4R",
        "outputId": "ec927266-8494-4632-e344-b66230cd6a74"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5414046121593291]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "zRWicPbr64pl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foz = 4\n",
        "imp_feature_size = 1\n",
        "n_estimators = 5\n",
        "imp_xgbc_acc = []\n",
        "xgbc_acc = []\n",
        "estimators_imp = []\n",
        "estimators = []\n",
        "train_embed = pd.DataFrame(train_embed)\n",
        "test_embed=pd.DataFrame(test_embed)\n",
        "d = len(train_embed.columns)\n",
        "\n",
        "for i in range(n_estimators):\n",
        "    imp_tr, imp_ts = generate_imp_space(train_embed, train_labels, test_embed, imWp_feature_size, foz)\n",
        "    imp_d = imp_tr.shape[1]\n",
        "\n",
        "    imp_sel_d = 2 * round(math.log2(imp_d))\n",
        "    sel_d = 2*round(math.log2(d))\n",
        "\n",
        "    imp_xgbc = xgb.XGBClassifier(max_features=imp_sel_d, n_estimators=50, random_state=42)\n",
        "    imp_xgbc.fit(imp_tr, train_labels)\n",
        "    estimators_imp.append((\"imp_xgbc\"+str(i), imp_rfc))\n",
        "\n",
        "    xgbc = xgb.XGBClassifier(max_features=sel_d, n_estimators=50, random_state=42)\n",
        "    xgbc.fit(train_embed, train_labels)\n",
        "    estimators.append((\"xgbc\"+str(i), rfc))\n",
        "\n",
        "\n",
        "voting_imp = VotingClassifier(estimators=estimators_imp)\n",
        "voting_imp.fit(imp_tr, train_labels)\n",
        "\n",
        "voting_xgb = VotingClassifier(estimators=estimators)\n",
        "voting_xgb.fit(train_embed, train_labels)\n",
        "\n",
        "imp_xgbc_acc = voting_imp.score(imp_ts, test_labels)\n",
        "xgbc_acc = voting_xgb.score(test_embed, test_labels)\n"
      ],
      "metadata": {
        "id": "5O_EYQnA6y73"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_xgbc_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFBgqHTd73uI",
        "outputId": "171d34fa-c2e6-46fa-9ffc-938df07ff83e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5400069881201957"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgbc_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0t5N1c174vt",
        "outputId": "c6cd0679-0f9a-4e15-884f-6316a2e39bda"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5414046121593291"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "65a922febd0a5868ed12620df141547b754242d19c66cf1750f84ba43717be07"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}